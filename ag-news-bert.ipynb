{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AN4ayYEV3kWoJXFfA0oypivgF7X2bhud",
      "authorship_tag": "ABX9TyOO2ttPMH3zMfqexykV2Nyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreassOlsson/HuggingFace/blob/main/ag-news-bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Hugging Face"
      ],
      "metadata": {
        "id": "h5Z09VsShmFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Download the English BERT from Huggingface’s Model Zoo\n",
        "2.   Download the AG News dataset\n",
        "3.   Prepare the dataset accordingly\n",
        "4.   Train the BERT model to do “News Topic Classification” using the training data.\n",
        "  1. Try to get the best possible test-score!\n"
      ],
      "metadata": {
        "id": "zpmG7VzDgRsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "bdSs8IDSlQNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "RNQM0feskInp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "W-oEWox1gTNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset by tokenizing the text and include padding to handle variable length sequences \n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "TS1PDhsmKT9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform datasets into TF dataset\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "training_set = tokenized_dataset['train'].shuffle(seed=42).select(range(50000))\n",
        "test_set = tokenized_dataset['test'].shuffle(seed=42).select(range(10000))\n",
        "\n",
        "tf_train_dataset = training_set.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = test_set.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")"
      ],
      "metadata": {
        "id": "YYCxLVOjl7qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
      ],
      "metadata": {
        "id": "24d87c7Y73Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")"
      ],
      "metadata": {
        "id": "VchliZAV8IyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "checkpoint_path = \"drive/MyDrive/Andreas Olsson/Huggingface/checkpoints/Large/\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Train the model with the new callback\n",
        "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=10, callbacks=[cp_callback])  # Pass callback to training"
      ],
      "metadata": {
        "id": "mrfSFroxkTD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}